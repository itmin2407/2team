{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸï¸ Motorcycle Night Ride - ë‹¤ì¤‘ ìš”ì¸ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "- **ë°ì´í„°ì…‹**: Acme AI Open Dataset - Motorcycle Night Ride (ì•¼ê°„ ì˜¤í† ë°”ì´ ì£¼í–‰, ~200í”„ë ˆì„)\n",
    "- **ëª©í‘œ**: ì„¸ë§Œí‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ + **ë‹¤ì¤‘ ìš”ì¸ ì•ˆì „ ì ìˆ˜(Multi-Factor Safety Score)** ì‚°ì¶œ\n",
    "- **6ê°œ í´ë˜ìŠ¤**: Undrivable, Road, Lanemark, My bike, Rider, Movable\n",
    "\n",
    "### ì‚¬ìš© ëª¨ë¸\n",
    "| ëª¨ë¸ | íŠ¹ì§• |\n",
    "|------|------|\n",
    "| **DeepLabV3+** | Atrous convolution + Encoder-Decoder, torchvision ì œê³µ |\n",
    "| **SegFormer** | Transformer ê¸°ë°˜, HuggingFace pretrained |\n",
    "| **BiSeNetV2** | ê²½ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë¸, Detail + Semantic ì´ì¤‘ ê²½ë¡œ |\n",
    "\n",
    "### ì•ˆì „ ì ìˆ˜ ì„¤ê³„: 4ê°€ì§€ ë…ë¦½ ìš”ì¸\n",
    "| ìš”ì¸ | ê°€ì¤‘ì¹˜ | ì˜ë¯¸ |\n",
    "|------|--------|------|\n",
    "| **ë„ë¡œ í™•ë³´ìœ¨** (Road Availability) | 40% | ì „ë°©ì— ì£¼í–‰ ê°€ëŠ¥í•œ ë„ë¡œê°€ ì¶©ë¶„í•œê°€? |\n",
    "| **ì¥ì• ë¬¼ ìœ„í—˜ë„** (Obstacle Risk) | 30% | ì „ë°© ë„ë¡œ ìœ„ì— ì¥ì• ë¬¼ì´ ìˆëŠ”ê°€? |\n",
    "| **ì°¨ì„  ê°€ì‹œì„±** (Lane Visibility) | 15% | ì°¨ì„ ì´ ë³´ì—¬ì„œ ìœ„ì¹˜ íŒŒì•…ì´ ê°€ëŠ¥í•œê°€? |\n",
    "| **ì‹œì•¼ í™•ë³´ìœ¨** (Visibility) | 15% | ì•¼ê°„ì— ì•ì´ ì¶©ë¶„íˆ ë³´ì´ëŠ”ê°€? |\n",
    "\n",
    "### ì¶”ê°€ ë¶„ì„\n",
    "- **GradCAM**: ê° í´ë˜ìŠ¤ ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹œ ì´ë¯¸ì§€ ì˜ì—­ ì‹œê°í™”\n",
    "- **Factor ë¶„í•´ ì‹œê°í™”**: ê° ìš”ì¸ì´ ì•ˆì „ ì ìˆ˜ì— ê¸°ì—¬í•˜ëŠ” ì •ë„ë¥¼ ë ˆì´ë” ì°¨íŠ¸ë¡œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "# ============================================================\n",
    "import subprocess, sys\n",
    "\n",
    "def install(pkg):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "packages = [\n",
    "    'torch', 'torchvision', 'torchaudio',\n",
    "    'transformers',\n",
    "    'pycocotools',\n",
    "    'pytorch-grad-cam',\n",
    "    'albumentations',\n",
    "    'numpy', 'pandas', 'matplotlib', 'seaborn', 'Pillow', 'scikit-learn'\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        install(pkg)\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ {pkg} ì„¤ì¹˜ ì‹¤íŒ¨: {e}')\n",
    "\n",
    "print('âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ & í™˜ê²½ ì„¤ì •\n",
    "# ============================================================\n",
    "import os, json, warnings, gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ (macOS)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ')\n",
    "print(f'ğŸ–¥ï¸  Device: {device}')\n",
    "print(f'ğŸ”¥ PyTorch: {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ë°ì´í„° EDA\n",
    "\n",
    "COCO í¬ë§· ì–´ë…¸í…Œì´ì…˜ì„ ë¡œë”©í•˜ê³ , ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ì—¬ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1-1. ë°ì´í„° ê²½ë¡œ ì„¤ì • & COCO JSON ë¡œë”©\n",
    "# ============================================================\n",
    "DATA_DIR = Path('..') / 'www.acmeai.tech ODataset 1 - Motorcycle Night Ride Dataset'\n",
    "IMAGE_DIR = DATA_DIR / 'images'\n",
    "JSON_PATH = DATA_DIR / 'COCO_motorcycle (pixel).json'\n",
    "\n",
    "# íŒŒì¼ ë¶„ë¥˜\n",
    "all_files = sorted(os.listdir(IMAGE_DIR))\n",
    "original_files = [f for f in all_files if not f.endswith('___fuse.png') and not f.endswith('___save.png')]\n",
    "mask_files_png = [f for f in all_files if f.endswith('___save.png')]\n",
    "fuse_files = [f for f in all_files if f.endswith('___fuse.png')]\n",
    "\n",
    "print(f'ğŸ“Š ì›ë³¸ ì´ë¯¸ì§€: {len(original_files)}ì¥')\n",
    "print(f'ğŸ“Š ë§ˆìŠ¤í¬ ì´ë¯¸ì§€: {len(mask_files_png)}ì¥')\n",
    "print(f'ğŸ“Š Fuse ì´ë¯¸ì§€: {len(fuse_files)}ì¥')\n",
    "\n",
    "# COCO API ë¡œë”©\n",
    "print('\\nâ³ COCO JSON ë¡œë”© ì¤‘ (200MB+, ì‹œê°„ ì†Œìš”)...')\n",
    "coco = COCO(str(JSON_PATH))\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ì •ë³´\n",
    "cat_ids = coco.getCatIds()\n",
    "cats = coco.loadCats(cat_ids)\n",
    "cat_id_to_name = {c['id']: c['name'] for c in cats}\n",
    "img_ids = coco.getImgIds()\n",
    "\n",
    "print(f'\\nâœ… ë¡œë”© ì™„ë£Œ!')\n",
    "print(f'ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜: {len(img_ids)}')\n",
    "print(f'ğŸ“ ì–´ë…¸í…Œì´ì…˜ ìˆ˜: {len(coco.getAnnIds())}')\n",
    "print(f'\\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬ ({len(cats)}ê°œ):')\n",
    "for c in cats:\n",
    "    print(f'   ID {c[\"id\"]}: {c[\"name\"]}')\n",
    "\n",
    "# í´ë˜ìŠ¤ ID ì¬ë§¤í•‘ (0=Background, 1~N=í´ë˜ìŠ¤)\n",
    "sorted_cat_ids = sorted(cat_id_to_name.keys())\n",
    "cat_remap = {0: 0}\n",
    "for new_id, orig_id in enumerate(sorted_cat_ids, start=1):\n",
    "    cat_remap[orig_id] = new_id\n",
    "\n",
    "num_classes = len(cat_remap)\n",
    "id_to_name = {0: 'Background'}\n",
    "for orig_id, new_id in cat_remap.items():\n",
    "    if orig_id != 0:\n",
    "        id_to_name[new_id] = cat_id_to_name[orig_id]\n",
    "class_names = [id_to_name[i] for i in range(num_classes)]\n",
    "\n",
    "print(f'\\nğŸ“– í´ë˜ìŠ¤ ë§¤í•‘ (ì´ {num_classes}ê°œ):')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'   {i}: {name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1-2. ë§ˆìŠ¤í¬ ìƒì„± í•¨ìˆ˜ + ìƒ˜í”Œ ì‹œê°í™” + í´ë˜ìŠ¤ ë¶„í¬\n",
    "# ============================================================\n",
    "def create_class_mask(coco_api, img_id, remap):\n",
    "    \"\"\"COCO ì–´ë…¸í…Œì´ì…˜ì—ì„œ í´ë˜ìŠ¤ ID ë§ˆìŠ¤í¬ ìƒì„±\"\"\"\n",
    "    img_info = coco_api.loadImgs(img_id)[0]\n",
    "    h, w = img_info['height'], img_info['width']\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    ann_ids = coco_api.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_api.loadAnns(ann_ids)\n",
    "    for ann in anns:\n",
    "        try:\n",
    "            binary_mask = coco_api.annToMask(ann)\n",
    "            new_id = remap.get(ann['category_id'], 0)\n",
    "            mask[binary_mask > 0] = new_id\n",
    "        except:\n",
    "            pass\n",
    "    return mask\n",
    "\n",
    "# ìƒ˜í”Œ ì‹œê°í™” (3ì¥)\n",
    "sample_ids = [img_ids[0], img_ids[len(img_ids)//3], img_ids[2*len(img_ids)//3]]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "\n",
    "for i, img_id in enumerate(sample_ids):\n",
    "    info = coco.loadImgs(img_id)[0]\n",
    "    fname = info['file_name']\n",
    "    img = Image.open(IMAGE_DIR / fname)\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f'ì›ë³¸: {fname}', fontsize=9)\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    mask = create_class_mask(coco, img_id, cat_remap)\n",
    "    axes[i, 1].imshow(mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "    axes[i, 1].set_title('Class Mask', fontsize=9)\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    fuse_path = IMAGE_DIR / (fname + '___fuse.png')\n",
    "    if fuse_path.exists():\n",
    "        axes[i, 2].imshow(Image.open(fuse_path))\n",
    "    axes[i, 2].set_title('Fuse (ì˜¤ë²„ë ˆì´)', fontsize=9)\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('ìƒ˜í”Œ: ì›ë³¸ / Class Mask / Fuse', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\n",
    "print('â³ ì „ì²´ ì´ë¯¸ì§€ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ì¤‘...')\n",
    "class_pixel_counts = Counter()\n",
    "total_pixels = 0\n",
    "for img_id in img_ids:\n",
    "    mask = create_class_mask(coco, img_id, cat_remap)\n",
    "    unique, counts = np.unique(mask, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        class_pixel_counts[u] += c\n",
    "    total_pixels += mask.size\n",
    "\n",
    "class_ratios = {class_names[k]: v / total_pixels for k, v in class_pixel_counts.items() if k < num_classes}\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names = list(class_ratios.keys())\n",
    "ratios = [class_ratios[n] for n in names]\n",
    "ax.barh(names, ratios, color=sns.color_palette('Set2', len(names)))\n",
    "ax.set_xlabel('ë©´ì  ë¹„ìœ¨')\n",
    "ax.set_title('ì „ì²´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë³„ ë©´ì  ë¹„ìœ¨')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for n, r in sorted(class_ratios.items(), key=lambda x: -x[1]):\n",
    "    print(f'  {n:>15s}: {r*100:.2f}%')\n",
    "print(f'\\nğŸ“ ì´ë¯¸ì§€ í•´ìƒë„: {img.size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "PyTorch Dataset/DataLoaderë¥¼ êµ¬ì„±í•˜ê³ , Albumentations ê¸°ë°˜ ë°ì´í„° ì¦ê°•ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "- COCO ì–´ë…¸í…Œì´ì…˜ â†’ í´ë˜ìŠ¤ ID ë§ˆìŠ¤í¬ (on-the-fly)\n",
    "- ì´ë¯¸ì§€ 512x512 ë¦¬ì‚¬ì´ì¦ˆ, ImageNet ì •ê·œí™”\n",
    "- Train 80% / Val 20% Stratified Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2-1. Dataset í´ë˜ìŠ¤ + Transforms + DataLoader\n",
    "# ============================================================\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 15\n",
    "LR = 1e-4\n",
    "\n",
    "class MotorcycleSegDataset(Dataset):\n",
    "    def __init__(self, coco_api, img_ids, image_dir, cat_remap, transform=None):\n",
    "        self.coco = coco_api\n",
    "        self.img_ids = img_ids\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.cat_remap = cat_remap\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        info = self.coco.loadImgs(img_id)[0]\n",
    "        img = np.array(Image.open(self.image_dir / info['file_name']).convert('RGB'))\n",
    "\n",
    "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        h, w = info['height'], info['width']\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        for ann in self.coco.loadAnns(ann_ids):\n",
    "            try:\n",
    "                bm = self.coco.annToMask(ann)\n",
    "                mask[bm > 0] = self.cat_remap.get(ann['category_id'], 0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if self.transform:\n",
    "            t = self.transform(image=img, mask=mask)\n",
    "            img, mask = t['image'], t['mask']\n",
    "\n",
    "        return img.float(), mask.long()\n",
    "\n",
    "# Albumentations ë³€í™˜\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Train / Val ë¶„í• \n",
    "train_ids, val_ids = train_test_split(img_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MotorcycleSegDataset(coco, train_ids, IMAGE_DIR, cat_remap, train_transform)\n",
    "val_dataset = MotorcycleSegDataset(coco, val_ids, IMAGE_DIR, cat_remap, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'ğŸ“Š Train: {len(train_dataset)}ì¥, Val: {len(val_dataset)}ì¥')\n",
    "print(f'ğŸ“Š Batch size: {BATCH_SIZE}, Image size: {IMG_SIZE}x{IMG_SIZE}')\n",
    "print(f'ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {num_classes} (Background í¬í•¨)')\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "sample_img, sample_mask = train_dataset[0]\n",
    "print(f'ğŸ“ Image shape: {sample_img.shape}, Mask shape: {sample_mask.shape}')\n",
    "print(f'ğŸ“ Mask unique values: {torch.unique(sample_mask).tolist()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ \n",
    "\n",
    "3ê°œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ fine-tuningí•˜ê³ , mIoU / Pixel Accuracyë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "- **DeepLabV3+**: ResNet-50 backbone, Atrous Spatial Pyramid Pooling\n",
    "- **SegFormer-B0**: Mix Transformer encoder, lightweight MLP decoder\n",
    "- **BiSeNetV2**: Detail Branch + Semantic Branch ì´ì¤‘ ê²½ë¡œ ì‹¤ì‹œê°„ ëª¨ë¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-0. í•™ìŠµ/í‰ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "# ============================================================\n",
    "def train_one_epoch(model, loader, criterion, optimizer, dev, model_type='standard'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in loader:\n",
    "        images, masks = images.to(dev), masks.to(dev)\n",
    "        if model_type == 'segformer':\n",
    "            out = model(pixel_values=images, labels=masks)\n",
    "            loss = out.loss\n",
    "        else:\n",
    "            out = model(images)\n",
    "            logits = out['out'] if isinstance(out, dict) else out\n",
    "            loss = criterion(logits, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def compute_miou(model, loader, n_classes, dev, model_type='standard'):\n",
    "    model.eval()\n",
    "    confusion = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(dev)\n",
    "            if model_type == 'segformer':\n",
    "                logits = model(pixel_values=images).logits\n",
    "                logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                out = model(images)\n",
    "                logits = out['out'] if isinstance(out, dict) else out\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "            gt = masks.numpy()\n",
    "            for p, g in zip(preds, gt):\n",
    "                valid = g < n_classes\n",
    "                confusion += np.bincount(\n",
    "                    n_classes * g[valid].astype(int) + p[valid].astype(int),\n",
    "                    minlength=n_classes**2\n",
    "                ).reshape(n_classes, n_classes)\n",
    "    iou = np.diag(confusion) / (confusion.sum(0) + confusion.sum(1) - np.diag(confusion) + 1e-10)\n",
    "    pixel_acc = np.diag(confusion).sum() / (confusion.sum() + 1e-10)\n",
    "    return np.nanmean(iou), iou, pixel_acc\n",
    "\n",
    "def train_model(model, name, loader_tr, loader_val, n_cls, dev,\n",
    "                epochs=NUM_EPOCHS, lr=LR, model_type='standard'):\n",
    "    \"\"\"ë²”ìš© í•™ìŠµ ë£¨í”„\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.to(dev)\n",
    "    history = []\n",
    "    print(f'\\nğŸ”¬ {name} í•™ìŠµ ì‹œì‘ ({epochs} epochs, lr={lr})')\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_one_epoch(model, loader_tr, criterion, optimizer, dev, model_type)\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            miou, iou_cls, pix_acc = compute_miou(model, loader_val, n_cls, dev, model_type)\n",
    "            history.append({'epoch': epoch+1, 'loss': loss, 'miou': miou, 'pix_acc': pix_acc})\n",
    "            print(f'  Epoch {epoch+1:3d}/{epochs}: Loss={loss:.4f}, mIoU={miou:.4f}, PixAcc={pix_acc:.4f}')\n",
    "    # ìµœì¢… í‰ê°€\n",
    "    miou, iou_cls, pix_acc = compute_miou(model, loader_val, n_cls, dev, model_type)\n",
    "    print(f'\\nğŸ“Š {name} ìµœì¢…: mIoU={miou:.4f}, PixelAcc={pix_acc:.4f}')\n",
    "    return model, miou, iou_cls, pix_acc, history\n",
    "\n",
    "print('âœ… í•™ìŠµ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš©\n",
    "all_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-1. Model 1: DeepLabV3+ (ResNet-50 backbone)\n",
    "# ============================================================\n",
    "# Pretrained on COCO â†’ fine-tune í—¤ë“œë¥¼ ìš°ë¦¬ í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ êµì²´\n",
    "deeplab = deeplabv3_resnet50(weights='COCO_WITH_VOC_LABELS_V1')\n",
    "deeplab.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "deeplab.aux_classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "deeplab, dl_miou, dl_iou, dl_pacc, dl_hist = train_model(\n",
    "    deeplab, 'DeepLabV3+', train_loader, val_loader, num_classes, device\n",
    ")\n",
    "all_results['DeepLabV3+'] = {\n",
    "    'model': deeplab, 'miou': dl_miou, 'iou_per_class': dl_iou,\n",
    "    'pixel_acc': dl_pacc, 'history': dl_hist, 'type': 'standard'\n",
    "}\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ IoU\n",
    "print('\\nğŸ“‹ DeepLabV3+ í´ë˜ìŠ¤ë³„ IoU:')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'  {name:>15s}: {dl_iou[i]:.4f}')\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-2. Model 2: SegFormer-B0 (HuggingFace Transformers)\n",
    "# ============================================================\n",
    "# Pretrained on ADE20K â†’ fine-tune for our classes\n",
    "segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    'nvidia/segformer-b0-finetuned-ade-512-512',\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "segformer, sf_miou, sf_iou, sf_pacc, sf_hist = train_model(\n",
    "    segformer, 'SegFormer-B0', train_loader, val_loader,\n",
    "    num_classes, device, model_type='segformer'\n",
    ")\n",
    "all_results['SegFormer-B0'] = {\n",
    "    'model': segformer, 'miou': sf_miou, 'iou_per_class': sf_iou,\n",
    "    'pixel_acc': sf_pacc, 'history': sf_hist, 'type': 'segformer'\n",
    "}\n",
    "\n",
    "print('\\nğŸ“‹ SegFormer-B0 í´ë˜ìŠ¤ë³„ IoU:')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'  {name:>15s}: {sf_iou[i]:.4f}')\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-3. BiSeNetV2 ì•„í‚¤í…ì²˜ ì •ì˜\n",
    "# ============================================================\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, ks=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, ks, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DetailBranch(nn.Module):\n",
    "    \"\"\"ê³µê°„ ì •ë³´ë¥¼ ë³´ì¡´í•˜ëŠ” ì–•ì€ ë¸Œëœì¹˜ (1/8 í•´ìƒë„)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.S1 = nn.Sequential(ConvBNReLU(3, 64, 3, 2, 1), ConvBNReLU(64, 64, 3, 1, 1))\n",
    "        self.S2 = nn.Sequential(ConvBNReLU(64, 64, 3, 2, 1), ConvBNReLU(64, 64, 3, 1, 1))\n",
    "        self.S3 = nn.Sequential(ConvBNReLU(64, 128, 3, 2, 1), ConvBNReLU(128, 128, 3, 1, 1))\n",
    "    def forward(self, x):\n",
    "        return self.S3(self.S2(self.S1(x)))\n",
    "\n",
    "class SemanticBranch(nn.Module):\n",
    "    \"\"\"ì˜ë¯¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¹Šì€ ë¸Œëœì¹˜ (ResNet-18 backbone, 1/32)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.compress = ConvBNReLU(512, 128, 1, 1, 0)\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return self.compress(x)\n",
    "\n",
    "class BiSeNetV2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.detail = DetailBranch()\n",
    "        self.semantic = SemanticBranch()\n",
    "        # ë‘ ë¸Œëœì¹˜ë¥¼ í•©ì¹˜ëŠ” Aggregation\n",
    "        self.agg_detail = ConvBNReLU(128, 128, 3, 1, 1)\n",
    "        self.agg_semantic = ConvBNReLU(128, 128, 3, 1, 1)\n",
    "        self.agg_out = ConvBNReLU(128, 128, 3, 1, 1)\n",
    "        self.head = nn.Sequential(ConvBNReLU(128, 64, 3, 1, 1), nn.Conv2d(64, num_classes, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        detail = self.detail(x)\n",
    "        semantic = self.semantic(x)\n",
    "        # Semanticì„ Detail í•´ìƒë„ì— ë§ì¶° ì—…ìƒ˜í”Œë§\n",
    "        sem_up = F.interpolate(semantic, size=detail.shape[2:], mode='bilinear', align_corners=False)\n",
    "        agg = self.agg_out(self.agg_detail(detail) + self.agg_semantic(sem_up))\n",
    "        out = self.head(agg)\n",
    "        return F.interpolate(out, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "print('âœ… BiSeNetV2 ì•„í‚¤í…ì²˜ ì •ì˜ ì™„ë£Œ')\n",
    "bisenet = BiSeNetV2(num_classes)\n",
    "total_params = sum(p.numel() for p in bisenet.parameters()) / 1e6\n",
    "print(f'ğŸ“Š BiSeNetV2 íŒŒë¼ë¯¸í„°: {total_params:.1f}M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-4. Model 3: BiSeNetV2 í•™ìŠµ\n",
    "# ============================================================\n",
    "bisenet, bs_miou, bs_iou, bs_pacc, bs_hist = train_model(\n",
    "    bisenet, 'BiSeNetV2', train_loader, val_loader, num_classes, device\n",
    ")\n",
    "all_results['BiSeNetV2'] = {\n",
    "    'model': bisenet, 'miou': bs_miou, 'iou_per_class': bs_iou,\n",
    "    'pixel_acc': bs_pacc, 'history': bs_hist, 'type': 'standard'\n",
    "}\n",
    "\n",
    "print('\\nğŸ“‹ BiSeNetV2 í´ë˜ìŠ¤ë³„ IoU:')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'  {name:>15s}: {bs_iou[i]:.4f}')\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-5. ëª¨ë¸ ë¹„êµ ìš”ì•½\n",
    "# ============================================================\n",
    "# ë¹„êµ í…Œì´ë¸”\n",
    "comp_data = []\n",
    "for name, res in all_results.items():\n",
    "    comp_data.append({'Model': name, 'mIoU': res['miou'], 'Pixel Acc': res['pixel_acc']})\n",
    "comp_df = pd.DataFrame(comp_data).sort_values('mIoU', ascending=False)\n",
    "print('ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:')\n",
    "display(comp_df)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "x = range(len(comp_df))\n",
    "axes[0].bar(x, comp_df['mIoU'], color=['#3498db', '#e74c3c', '#2ecc71'][:len(comp_df)])\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comp_df['Model'])\n",
    "axes[0].set_ylabel('mIoU')\n",
    "axes[0].set_title('ëª¨ë¸ë³„ mIoU ë¹„êµ')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(comp_df['mIoU']):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "axes[1].bar(x, comp_df['Pixel Acc'], color=['#3498db', '#e74c3c', '#2ecc71'][:len(comp_df)])\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comp_df['Model'])\n",
    "axes[1].set_ylabel('Pixel Accuracy')\n",
    "axes[1].set_title('ëª¨ë¸ë³„ Pixel Accuracy ë¹„êµ')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(comp_df['Pixel Acc']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.suptitle('ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ IoU ë¹„êµ íˆíŠ¸ë§µ\n",
    "iou_df = pd.DataFrame({name: res['iou_per_class'] for name, res in all_results.items()}, index=class_names)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(iou_df, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax, vmin=0, vmax=1)\n",
    "ax.set_title('ëª¨ë¸ë³„ í´ë˜ìŠ¤ IoU ë¹„êµ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best model ì„ íƒ\n",
    "best_name = comp_df.iloc[0]['Model']\n",
    "best_model = all_results[best_name]['model']\n",
    "best_type = all_results[best_name]['type']\n",
    "print(f'\\nğŸ† Best Model: {best_name} (mIoU={all_results[best_name][\"miou\"]:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ë‹¤ì¤‘ ìš”ì¸ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\n",
    "\n",
    "ë‹¨ìˆœ ê°€ì¤‘í•© ëŒ€ì‹ , **ìš´ì „ ì•ˆì „ì˜ 4ê°€ì§€ ë…ë¦½ ìš”ì¸**ì„ ë¶„í•´í•˜ì—¬ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ì„¤ê³„ ë…¼ë¦¬\n",
    "\n",
    "**Factor 1: ë„ë¡œ í™•ë³´ìœ¨ (40%)**\n",
    "> ìê¸° ì°¨ëŸ‰ì„ ì œì™¸í•œ ì‹œì•¼ì—ì„œ (Road + Lanemark) ë¹„ìœ¨. 50% ì´ìƒì´ë©´ ë§Œì .\n",
    "\n",
    "**Factor 2: ì¥ì• ë¬¼ ìœ„í—˜ë„ (30%)**\n",
    "> ì´ë¯¸ì§€ **í•˜ë‹¨ 50%** (= ì¹´ë©”ë¼ì— ê°€ê¹Œìš´ ì „ë°©)ì—ì„œ Movable ë°€ë„. ê³µê°„ì  ìœ„ì¹˜ë¥¼ ë°˜ì˜.\n",
    "\n",
    "**Factor 3: ì°¨ì„  ê°€ì‹œì„± (15%)**\n",
    "> Lanemark ë¹„ìœ¨ì´ ê¸°ëŒ€ì¹˜(2%) ì´ìƒì´ë©´ ë§Œì . ì•¼ê°„ì— ì°¨ì„  ë¯¸ì¸ì‹ = ì´íƒˆ ìœ„í—˜.\n",
    "\n",
    "**Factor 4: ì‹œì•¼ í™•ë³´ìœ¨ (15%)**\n",
    "> Background(ì–´ë‘ ) ì™¸ ì‹ë³„ ê°€ëŠ¥ ì˜ì—­ ë¹„ìœ¨. 70% ì´ìƒ ì‹ë³„ë˜ë©´ ë§Œì .\n",
    "\n",
    "$$\\text{Safety} = 0.40 \\times F_\\text{road} + 0.30 \\times F_\\text{obstacle} + 0.15 \\times F_\\text{lane} + 0.15 \\times F_\\text{visibility}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4-1. ë‹¤ì¤‘ ìš”ì¸ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\n",
    "# ============================================================\n",
    "name_to_idx = {n: i for i, n in enumerate(class_names)}\n",
    "\n",
    "def predict_mask(model, img_tensor, dev, model_type='standard'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = img_tensor.unsqueeze(0).to(dev)\n",
    "        if model_type == 'segformer':\n",
    "            logits = model(pixel_values=x).logits\n",
    "            logits = F.interpolate(logits, size=img_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            out = model(x)\n",
    "            logits = out['out'] if isinstance(out, dict) else out\n",
    "    return logits.argmax(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "def compute_multifactor_safety(pred_mask, name_to_idx):\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ ìš”ì¸ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\n",
    "    Returns: (ì´ì  0~1, Factor dict, í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ dict)\n",
    "    \"\"\"\n",
    "    H, W = pred_mask.shape\n",
    "    total = H * W\n",
    "\n",
    "    # í´ë˜ìŠ¤ë³„ í”½ì…€ ìˆ˜\n",
    "    def px(name):\n",
    "        idx = name_to_idx.get(name, -1)\n",
    "        return int(np.sum(pred_mask == idx)) if idx >= 0 else 0\n",
    "\n",
    "    road_px = px('Road')\n",
    "    lane_px = px('Lanemark')\n",
    "    undrivable_px = px('Undrivable')\n",
    "    movable_px = px('Movable')\n",
    "    mybike_px = px('My bike')\n",
    "    rider_px = px('Rider')\n",
    "    bg_px = px('Background')\n",
    "\n",
    "    # ---- Factor 1: ë„ë¡œ í™•ë³´ìœ¨ (Road Availability) ----\n",
    "    # ìê¸° ì°¨ëŸ‰ ì œì™¸ ì˜ì—­ ëŒ€ë¹„ ì£¼í–‰ ê°€ëŠ¥(Road+Lanemark) ë¹„ìœ¨\n",
    "    non_self = total - mybike_px - rider_px\n",
    "    drivable_ratio = (road_px + lane_px) / (non_self + 1e-10)\n",
    "    f_road = min(drivable_ratio / 0.5, 1.0)  # 50% ì´ìƒì´ë©´ ë§Œì \n",
    "\n",
    "    # ---- Factor 2: ì¥ì• ë¬¼ ìœ„í—˜ë„ (Obstacle Risk) ----\n",
    "    # ì´ë¯¸ì§€ í•˜ë‹¨ 50% (ì „ë°© ê·¼ê±°ë¦¬)ì—ì„œ Movable ë°€ë„\n",
    "    lower_half = pred_mask[H // 2:, :]\n",
    "    movable_idx = name_to_idx.get('Movable', -1)\n",
    "    movable_in_lower = int(np.sum(lower_half == movable_idx)) if movable_idx >= 0 else 0\n",
    "    lower_total = lower_half.size\n",
    "    obstacle_density = movable_in_lower / (lower_total + 1e-10)\n",
    "    f_obstacle = max(1.0 - obstacle_density * 10, 0.0)  # 10% ì´ìƒì´ë©´ 0ì \n",
    "\n",
    "    # ---- Factor 3: ì°¨ì„  ê°€ì‹œì„± (Lane Visibility) ----\n",
    "    lane_ratio = lane_px / (total + 1e-10)\n",
    "    f_lane = min(lane_ratio / 0.02, 1.0)  # 2% ì´ìƒì´ë©´ ë§Œì \n",
    "\n",
    "    # ---- Factor 4: ì‹œì•¼ í™•ë³´ìœ¨ (Visibility) ----\n",
    "    identified_ratio = (total - bg_px) / (total + 1e-10)\n",
    "    f_visibility = min(identified_ratio / 0.7, 1.0)  # 70% ì´ìƒ ì‹ë³„ì´ë©´ ë§Œì \n",
    "\n",
    "    # ---- ìµœì¢… ì ìˆ˜ ----\n",
    "    safety = 0.40 * f_road + 0.30 * f_obstacle + 0.15 * f_lane + 0.15 * f_visibility\n",
    "\n",
    "    factors = {\n",
    "        'ë„ë¡œ í™•ë³´ìœ¨': round(f_road, 4),\n",
    "        'ì¥ì• ë¬¼ ìœ„í—˜ë„': round(f_obstacle, 4),\n",
    "        'ì°¨ì„  ê°€ì‹œì„±': round(f_lane, 4),\n",
    "        'ì‹œì•¼ í™•ë³´ìœ¨': round(f_visibility, 4),\n",
    "    }\n",
    "    ratios = {\n",
    "        'Road': road_px / total, 'Lanemark': lane_px / total,\n",
    "        'Undrivable': undrivable_px / total, 'Movable': movable_px / total,\n",
    "        'Background': bg_px / total,\n",
    "    }\n",
    "    return round(safety, 4), factors, ratios\n",
    "\n",
    "# ---- ì „ì²´ Val ì´ë¯¸ì§€ì— ëŒ€í•´ ì‚°ì¶œ ----\n",
    "print(f'â³ {best_name}ìœ¼ë¡œ ë‹¤ì¤‘ ìš”ì¸ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ ì¤‘...')\n",
    "safety_records = []\n",
    "\n",
    "for img_id in val_ids:\n",
    "    info = coco.loadImgs(img_id)[0]\n",
    "    img = np.array(Image.open(IMAGE_DIR / info['file_name']).convert('RGB'))\n",
    "    t = val_transform(image=img)\n",
    "    img_tensor = t['image'].float()\n",
    "    pred = predict_mask(best_model, img_tensor, device, best_type)\n",
    "    score, factors, ratios = compute_multifactor_safety(pred, name_to_idx)\n",
    "    safety_records.append({\n",
    "        'image_id': img_id, 'file_name': info['file_name'],\n",
    "        'safety_score': round(score * 100, 1), **factors, **ratios\n",
    "    })\n",
    "\n",
    "safety_df = pd.DataFrame(safety_records)\n",
    "safety_df['grade'] = safety_df['safety_score'].apply(\n",
    "    lambda s: 'Safe' if s >= 66 else ('Caution' if s >= 33 else 'Dangerous')\n",
    ")\n",
    "\n",
    "print(f'âœ… ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ ì™„ë£Œ ({len(safety_df)}ì¥)')\n",
    "print(f'\\nğŸ“Š ì•ˆì „ ì ìˆ˜ í†µê³„:')\n",
    "print(safety_df['safety_score'].describe().round(1))\n",
    "print(f'\\nğŸ“Š ë“±ê¸‰ ë¶„í¬:')\n",
    "print(safety_df['grade'].value_counts())\n",
    "\n",
    "# ---- ì‹œê°í™” 1: ì ìˆ˜ ë¶„í¬ + ë“±ê¸‰ + ì¶”ì´ ----\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].hist(safety_df['safety_score'], bins=15, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(33, color='red', linestyle='--')\n",
    "axes[0].axvline(66, color='green', linestyle='--')\n",
    "axes[0].set_title('ì•ˆì „ ì ìˆ˜ ë¶„í¬')\n",
    "axes[0].set_xlabel('ì ìˆ˜')\n",
    "\n",
    "grade_counts = safety_df['grade'].value_counts()\n",
    "colors_pie = {'Safe': '#2ecc71', 'Caution': '#f39c12', 'Dangerous': '#e74c3c'}\n",
    "axes[1].pie(grade_counts.values, labels=grade_counts.index,\n",
    "            autopct='%1.1f%%', colors=[colors_pie.get(g, 'gray') for g in grade_counts.index])\n",
    "axes[1].set_title('ë“±ê¸‰ ë¶„í¬')\n",
    "\n",
    "axes[2].plot(range(len(safety_df)), safety_df['safety_score'].values, color='steelblue', alpha=0.7)\n",
    "axes[2].fill_between(range(len(safety_df)), 0, 33, alpha=0.1, color='red')\n",
    "axes[2].fill_between(range(len(safety_df)), 33, 66, alpha=0.1, color='orange')\n",
    "axes[2].fill_between(range(len(safety_df)), 66, 100, alpha=0.1, color='green')\n",
    "axes[2].set_title('í”„ë ˆì„ë³„ ì•ˆì „ ì ìˆ˜')\n",
    "axes[2].set_xlabel('í”„ë ˆì„')\n",
    "axes[2].set_ylabel('ì ìˆ˜')\n",
    "\n",
    "plt.suptitle(f'ë‹¤ì¤‘ ìš”ì¸ ì•ˆì „ ì ìˆ˜ ({best_name})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- ì‹œê°í™” 2: Factor ë¶„í•´ ë¶„ì„ (ë ˆì´ë” ì°¨íŠ¸) ----\n",
    "factor_cols = ['ë„ë¡œ í™•ë³´ìœ¨', 'ì¥ì• ë¬¼ ìœ„í—˜ë„', 'ì°¨ì„  ê°€ì‹œì„±', 'ì‹œì•¼ í™•ë³´ìœ¨']\n",
    "factor_weights = [0.40, 0.30, 0.15, 0.15]\n",
    "\n",
    "# ë“±ê¸‰ë³„ í‰ê·  Factor\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), subplot_kw=dict(polar=True))\n",
    "angles = np.linspace(0, 2 * np.pi, len(factor_cols), endpoint=False).tolist()\n",
    "angles += angles[:1]  # ë‹«ê¸°\n",
    "\n",
    "for ax, grade in zip(axes, ['Safe', 'Caution', 'Dangerous']):\n",
    "    subset = safety_df[safety_df['grade'] == grade]\n",
    "    if len(subset) == 0:\n",
    "        ax.set_title(f'{grade} (í•´ë‹¹ ì—†ìŒ)')\n",
    "        continue\n",
    "    means = [subset[c].mean() for c in factor_cols]\n",
    "    means += means[:1]\n",
    "    ax.fill(angles, means, alpha=0.25,\n",
    "            color=colors_pie.get(grade, 'gray'))\n",
    "    ax.plot(angles, means, 'o-', linewidth=2,\n",
    "            color=colors_pie.get(grade, 'gray'))\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(factor_cols, fontsize=9)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f'{grade} (n={len(subset)})', fontsize=12, fontweight='bold', pad=15)\n",
    "\n",
    "plt.suptitle('ë“±ê¸‰ë³„ ì•ˆì „ ìš”ì¸ ë ˆì´ë” ì°¨íŠ¸', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- ì‹œê°í™” 3: Factor ê¸°ì—¬ë„ Stacked Bar ----\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "bottom = np.zeros(len(safety_df))\n",
    "colors_f = ['#3498db', '#e74c3c', '#f39c12', '#2ecc71']\n",
    "for ci, col in enumerate(factor_cols):\n",
    "    weighted = safety_df[col].values * factor_weights[ci] * 100\n",
    "    ax.bar(range(len(safety_df)), weighted, bottom=bottom,\n",
    "           label=f'{col} (x{factor_weights[ci]})', color=colors_f[ci], alpha=0.8, width=1.0)\n",
    "    bottom += weighted\n",
    "ax.axhline(33, color='red', linestyle='--', alpha=0.5)\n",
    "ax.axhline(66, color='green', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('í”„ë ˆì„')\n",
    "ax.set_ylabel('ì•ˆì „ ì ìˆ˜ ê¸°ì—¬ë„')\n",
    "ax.set_title('í”„ë ˆì„ë³„ Factor ê¸°ì—¬ë„ ë¶„í•´')\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Factor í†µê³„ ----\n",
    "print('\\nğŸ“Š Factorë³„ í†µê³„:')\n",
    "display(safety_df[factor_cols].describe().round(3))\n",
    "\n",
    "print('\\nğŸ“Œ Factor í•´ì„:')\n",
    "for col in factor_cols:\n",
    "    avg = safety_df[col].mean()\n",
    "    status = 'ğŸŸ¢ ì–‘í˜¸' if avg >= 0.7 else ('ğŸŸ¡ ì£¼ì˜' if avg >= 0.4 else 'ğŸ”´ ìœ„í—˜')\n",
    "    print(f'  {col:>12s}: í‰ê·  {avg:.3f} {status}')\n",
    "\n",
    "# ---- Safe / Dangerous ëŒ€í‘œ í”„ë ˆì„ ----\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "for i, (_, row) in enumerate(safety_df.nlargest(3, 'safety_score').iterrows()):\n",
    "    fuse_p = IMAGE_DIR / (row['file_name'] + '___fuse.png')\n",
    "    orig_p = IMAGE_DIR / row['file_name']\n",
    "    p = fuse_p if fuse_p.exists() else orig_p\n",
    "    if p.exists():\n",
    "        axes[0, i].imshow(Image.open(p))\n",
    "    axes[0, i].set_title(f'Safe: {row[\"safety_score\"]}ì ', color='green', fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, (_, row) in enumerate(safety_df.nsmallest(3, 'safety_score').iterrows()):\n",
    "    fuse_p = IMAGE_DIR / (row['file_name'] + '___fuse.png')\n",
    "    orig_p = IMAGE_DIR / row['file_name']\n",
    "    p = fuse_p if fuse_p.exists() else orig_p\n",
    "    if p.exists():\n",
    "        axes[1, i].imshow(Image.open(p))\n",
    "    axes[1, i].set_title(f'Dangerous: {row[\"safety_score\"]}ì ', color='red', fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('ì•ˆì „ ë“±ê¸‰ ë¹„êµ (ìƒìœ„ 3 vs í•˜ìœ„ 3)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. GradCAM ì‹œê°í™”\n",
    "\n",
    "**GradCAM** (Gradient-weighted Class Activation Mapping)ì„ í†µí•´ ê° ëª¨ë¸ì´ íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì´ë¯¸ì§€ì˜ **ì–´ëŠ ì˜ì—­ì— ì£¼ëª©**í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "- ê° ëª¨ë¸ì˜ ë§ˆì§€ë§‰ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´ì— GradCAM ì ìš©\n",
    "- Road, Movable, Undrivable ë“± ì•ˆì „ì— í•µì‹¬ì ì¸ í´ë˜ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë¶„ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5-1. GradCAM ì‹œê°í™”\n",
    "# ============================================================\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import SemanticSegmentationTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# ---- ëª¨ë¸ë³„ íƒ€ê²Ÿ ë ˆì´ì–´ ì„¤ì • ----\n",
    "def get_target_layer(model, model_name):\n",
    "    \"\"\"ê° ëª¨ë¸ì˜ GradCAM íƒ€ê²Ÿ ë ˆì´ì–´ ë°˜í™˜\"\"\"\n",
    "    if 'DeepLab' in model_name:\n",
    "        return [model.backbone.layer4[-1]]\n",
    "    elif 'SegFormer' in model_name:\n",
    "        # SegFormer encoderì˜ ë§ˆì§€ë§‰ ë¸”ë¡\n",
    "        return [model.segformer.encoder.block[-1][-1].output.dense]\n",
    "    elif 'BiSeNet' in model_name:\n",
    "        return [model.semantic.layer4[-1]]\n",
    "    return []\n",
    "\n",
    "# ---- GradCAMìš© ëª¨ë¸ ë˜í¼ (SegFormerìš©) ----\n",
    "class SegformerWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        out = self.model(pixel_values=x)\n",
    "        logits = out.logits\n",
    "        return F.interpolate(logits, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "# ---- ì‹œê°í™” í•¨ìˆ˜ ----\n",
    "def denormalize(img_tensor):\n",
    "    \"\"\"ImageNet ì •ê·œí™” ì—­ë³€í™˜ â†’ [0,1] numpy\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = img * std + mean\n",
    "    return np.clip(img, 0, 1).astype(np.float32)\n",
    "\n",
    "# ì‹œê°í™” ëŒ€ìƒ í´ë˜ìŠ¤ (ì•ˆì „ì— í•µì‹¬ì ì¸ í´ë˜ìŠ¤)\n",
    "target_class_names = ['Road', 'Movable', 'Undrivable']\n",
    "target_class_ids = [name_to_idx.get(n, 0) for n in target_class_names if n in name_to_idx]\n",
    "target_class_names = [n for n in target_class_names if n in name_to_idx]\n",
    "\n",
    "# ìƒ˜í”Œ ì´ë¯¸ì§€ 3ì¥ ì¤€ë¹„\n",
    "sample_val_ids = val_ids[:3]\n",
    "sample_tensors = []\n",
    "sample_rgb = []\n",
    "for img_id in sample_val_ids:\n",
    "    info = coco.loadImgs(img_id)[0]\n",
    "    img = np.array(Image.open(IMAGE_DIR / info['file_name']).convert('RGB'))\n",
    "    t = val_transform(image=img)\n",
    "    sample_tensors.append(t['image'].float())\n",
    "    sample_rgb.append(denormalize(t['image'].float()))\n",
    "\n",
    "# ---- ëª¨ë¸ë³„ GradCAM ì‹œê°í™” ----\n",
    "for model_name, res in all_results.items():\n",
    "    mdl = res['model']\n",
    "    mtype = res['type']\n",
    "\n",
    "    try:\n",
    "        if mtype == 'segformer':\n",
    "            wrapped = SegformerWrapper(mdl)\n",
    "            target_layers = [mdl.segformer.encoder.block[-1][-1].output.dense]\n",
    "            cam_model = wrapped\n",
    "        else:\n",
    "            target_layers = get_target_layer(mdl, model_name)\n",
    "            cam_model = mdl\n",
    "\n",
    "        cam_model.eval()\n",
    "        cam = GradCAM(model=cam_model, target_layers=target_layers)\n",
    "\n",
    "        n_classes_show = len(target_class_names)\n",
    "        n_samples = len(sample_tensors)\n",
    "        fig, axes = plt.subplots(n_samples, n_classes_show + 1, figsize=(5 * (n_classes_show + 1), 5 * n_samples))\n",
    "        if n_samples == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "\n",
    "        for si in range(n_samples):\n",
    "            # ì›ë³¸ ì´ë¯¸ì§€\n",
    "            axes[si, 0].imshow(sample_rgb[si])\n",
    "            axes[si, 0].set_title('ì›ë³¸', fontsize=10)\n",
    "            axes[si, 0].axis('off')\n",
    "\n",
    "            input_tensor = sample_tensors[si].unsqueeze(0)\n",
    "\n",
    "            # ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ì–»ê¸°\n",
    "            with torch.no_grad():\n",
    "                if mtype == 'segformer':\n",
    "                    pred = cam_model(input_tensor.to(device)).argmax(1).squeeze(0).cpu().numpy()\n",
    "                else:\n",
    "                    out = mdl(input_tensor.to(device))\n",
    "                    logits = out['out'] if isinstance(out, dict) else out\n",
    "                    pred = logits.argmax(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "            for ci, (cls_name, cls_id) in enumerate(zip(target_class_names, target_class_ids)):\n",
    "                # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë§ˆìŠ¤í¬ ì˜ì—­\n",
    "                cls_mask = (pred == cls_id).astype(np.float32)\n",
    "                if cls_mask.sum() < 10:\n",
    "                    axes[si, ci + 1].imshow(sample_rgb[si])\n",
    "                    axes[si, ci + 1].set_title(f'{cls_name}\\n(ë¯¸ê²€ì¶œ)', fontsize=10)\n",
    "                    axes[si, ci + 1].axis('off')\n",
    "                    continue\n",
    "\n",
    "                targets = [SemanticSegmentationTarget(cls_id, torch.from_numpy(cls_mask))]\n",
    "                grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "                cam_image = show_cam_on_image(sample_rgb[si], grayscale_cam, use_rgb=True)\n",
    "\n",
    "                axes[si, ci + 1].imshow(cam_image)\n",
    "                axes[si, ci + 1].set_title(f'{cls_name}', fontsize=10)\n",
    "                axes[si, ci + 1].axis('off')\n",
    "\n",
    "        plt.suptitle(f'GradCAM: {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f'âœ… {model_name} GradCAM ì™„ë£Œ')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ {model_name} GradCAM ì‹¤íŒ¨: {e}')\n",
    "\n",
    "print('\\nğŸ“Œ GradCAM í•´ì„:')\n",
    "print('  - ë¹¨ê°„ ì˜ì—­: í•´ë‹¹ í´ë˜ìŠ¤ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ ë¶€ë¶„')\n",
    "print('  - Road: ë„ë¡œ í‘œë©´ ìì²´ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸')\n",
    "print('  - Movable: ì°¨ëŸ‰/ë³´í–‰ì ì˜ì—­ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸')\n",
    "print('  - Undrivable: ì¸ë„/ê±´ë¬¼ ë“± ë¹„ì£¼í–‰ ì˜ì—­ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. ê²°ë¡ \n",
    "\n",
    "## 6-1. í”„ë¡œì íŠ¸ ìš”ì•½\n",
    "- **ë°ì´í„°**: Motorcycle Night Ride ë°ì´í„°ì…‹ (~200ì¥, 6í´ë˜ìŠ¤ ì„¸ë§Œí‹± ì„¸ê·¸ë©˜í…Œì´ì…˜)\n",
    "- **ëª¨ë¸**: DeepLabV3+, SegFormer-B0, BiSeNetV2 ì„¸ ê°€ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ fine-tuning\n",
    "- **ì•ˆì „ ì ìˆ˜**: ë‹¨ìˆœ ê°€ì¤‘í•©ì´ ì•„ë‹Œ **4ê°€ì§€ ë…ë¦½ ìš”ì¸ ê¸°ë°˜ ë‹¤ì¤‘ ì•ˆì „ ì ìˆ˜** ì‚°ì¶œ\n",
    "- **XAI**: GradCAMìœ¼ë¡œ ëª¨ë¸ì˜ í´ë˜ìŠ¤ë³„ ì£¼ëª© ì˜ì—­ ì‹œê°í™”\n",
    "\n",
    "## 6-2. ì•ˆì „ ì ìˆ˜ ì„¤ê³„ì˜ í•µì‹¬ ì°¨ë³„ì \n",
    "| ê¸°ì¡´ ë°©ì‹ (ë‹¨ìˆœ ê°€ì¤‘í•©) | ë³¸ í”„ë¡œì íŠ¸ (ë‹¤ì¤‘ ìš”ì¸) |\n",
    "|------------------------|----------------------|\n",
    "| ì „ì²´ ì´ë¯¸ì§€ ë©´ì ë§Œ ê³ ë ¤ | **ê³µê°„ì  ìœ„ì¹˜** ë°˜ì˜ (í•˜ë‹¨ ROI = ì „ë°©) |\n",
    "| ê°€ì¤‘ì¹˜ì— ë„ë©”ì¸ ê·¼ê±° ì•½í•¨ | **ìš´ì „ ì•ˆì „ì˜ 4ê°€ì§€ í•µì‹¬ ìš”ì¸** ë¶„í•´ |\n",
    "| ê²°ê³¼ í•´ì„ì´ ì–´ë ¤ì›€ | **Factorë³„ ê¸°ì—¬ë„** ë¶„í•´ ê°€ëŠ¥ |\n",
    "| My bike/Rider/Background ë¬´ì‹œ | **ìê¸° ì°¨ëŸ‰ ì œì™¸, ì‹œì•¼ í™•ë³´ìœ¨** ë°˜ì˜ |\n",
    "\n",
    "## 6-3. ì£¼ìš” ë°œê²¬\n",
    "1. **ë„ë¡œ í™•ë³´ìœ¨**ì´ ì•ˆì „ ì ìˆ˜ì— ê°€ì¥ í° ì˜í–¥ (40%) â€” ì „ë°© ë„ë¡œ ë„“ì´ê°€ í•µì‹¬\n",
    "2. **ì¥ì• ë¬¼ ìœ„í—˜ë„**ëŠ” ì „ë°©(í•˜ë‹¨ ROI)ì— Movableì´ ìˆì„ ë•Œë§Œ ê°ì  â€” ê³µê°„ ì¸ì‹ ë°˜ì˜\n",
    "3. **ì•¼ê°„ ì‹œì•¼ í™•ë³´ìœ¨**ì´ ì˜ì™¸ë¡œ ë³€ë™ì´ í° ìš”ì¸ â€” ì¡°ëª…/í™˜ê²½ì— ë”°ë¼ í¬ê²Œ ë³€í™”\n",
    "4. ë ˆì´ë” ì°¨íŠ¸ë¡œ **ë“±ê¸‰ë³„ ì•½í•œ ìš”ì¸**ì„ ì§ê´€ì ìœ¼ë¡œ íŒŒì•… ê°€ëŠ¥\n",
    "\n",
    "## 6-4. Metric ê·¼ê±°\n",
    "- **mIoU / Pixel Accuracy**: ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ìì²´ì˜ ì„±ëŠ¥ í‰ê°€\n",
    "- **ë‹¤ì¤‘ ìš”ì¸ Safety Score**: 4ê°œ ìš”ì¸ Ã— ë„ë©”ì¸ ê°€ì¤‘ì¹˜ â†’ ì •ëŸ‰ì  ì•ˆì „ í‰ê°€\n",
    "- **Factor ë¶„í•´**: ì–´ë–¤ ìš”ì¸ ë•Œë¬¸ì— ìœ„í—˜í•œì§€ **í•´ì„ ê°€ëŠ¥í•œ ì ìˆ˜** ì œê³µ\n",
    "\n",
    "## 6-5. í•œê³„ ë° í–¥í›„ ê³¼ì œ\n",
    "- **ë°ì´í„° ìˆ˜ ë¶€ì¡± (~200ì¥)**: ë” í° ë°ì´í„°ì…‹ê³¼ ê²°í•© ì‹œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€\n",
    "- **ì•¼ê°„ ì „ìš©**: ì£¼ê°„/ì•…ì²œí›„ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ í™˜ê²½ ë°ì´í„° í•„ìš”\n",
    "- **ë‹¨ì¼ í”„ë ˆì„ ë¶„ì„**: ì˜ìƒ ì‹œê³„ì—´ ì •ë³´ í™œìš© ì‹œ ë” ì •êµí•œ ì•ˆì „ í‰ê°€ ê°€ëŠ¥\n",
    "- **Factor ê°€ì¤‘ì¹˜ ìµœì í™”**: ì‹¤ì œ ì‚¬ê³  ë°ì´í„°ì™€ ëŒ€ì¡°í•˜ì—¬ ê°€ì¤‘ì¹˜ ìë™ í•™ìŠµ ê°€ëŠ¥\n",
    "- **ROI ì„¸ë¶„í™”**: í•˜ë‹¨ 50% ì™¸ì— ì¢Œ/ìš°/ì¤‘ì•™ ë“± ë” ì„¸ë°€í•œ ê³µê°„ ë¶„í•  ê°€ëŠ¥"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
